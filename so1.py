import re
from collections import Counter


def analyze_text(text):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–ª–æ–≤–∞–º"""

    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    clean_text = re.sub(r'[^\w\s]', '', text.lower())

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    words = clean_text.split()

    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    word_count = len(words)
    unique_words = len(set(words))
    most_common = Counter(words).most_common(5)

    # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞
    avg_length = sum(len(word) for word in words) / len(words) if words else 0

    return {
        'total_words': word_count,
        'unique_words': unique_words,
        'most_common': most_common,
        'average_word_length': round(avg_length, 2)
    }


def print_analysis(stats):
    """–ö—Ä–∞—Å–∏–≤–æ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    print("üìä –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞:")
    print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {stats['total_words']}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {stats['unique_words']}")
    print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞: {stats['average_word_length']} —Å–∏–º–≤–æ–ª–æ–≤")
    print("\nüî• –¢–æ–ø-5 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤:")

    for word, count in stats['most_common']:
        print(f"  '{word}': {count} —Ä–∞–∑")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
sample_text = """
Python - —ç—Ç–æ –º–æ—â–Ω—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. Python –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏,
–∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –º–Ω–æ–≥–æ–≥–æ –¥—Ä—É–≥–æ–≥–æ. –ò–∑—É—á–µ–Ω–∏–µ Python –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç
–º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤.
"""

if __name__ == "__main__":
    stats = analyze_text(sample_text)
    print_analysis(stats)

    print("\n" + "=" * 50)
    print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤–≤–µ—Å—Ç–∏ —Å–≤–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!")
